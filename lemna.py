# -*- coding: utf-8 -*-
"""lemna.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v98GmeeGQyKPjCgmAlToyy-NBZSL3EmP
"""

# !pip install lime
from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, mean_squared_error, recall_score, roc_curve
from sklearn.utils import shuffle, check_random_state
from sklearn.model_selection import train_test_split
from sklearn.mixture import GaussianMixture
# from google.colab import drive, files
from keras.models import Sequential
from keras.layers import Dense
import pandas as pd 
import numpy as np 
import pickle
import lime
import lime.lime_tabular
import matplotlib.pyplot as plt

# drive.mount('/content/drive')

ben  = pd.read_csv('benign.csv')
mal  = pd.read_csv('malicious.csv')
data = ben.append(mal, ignore_index=True)
data = shuffle(data)

y = data['class']
x = data.drop(['class'], axis=1)
x = x.drop(['filename'],axis=1)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.3, random_state=27)

feature_means = []
feature_maxs = []

def categorical_helper_fun(x):
    if isinstance(x, int) or isinstance(x,float):
        return int(x)
    else:
        return 0

print (feature_maxs)

for i in data:
    if True in data[i]:
        data[i] = list(map(lambda x: categorical_helper_fun(x), data[i]))
    feature_means.append(np.mean(data[i]))
    feature_maxs.append(max(data[i]))

feature_names = []
for i in data:
    feature_names.append(i)
print (feature_names)

l = lime.lime_tabular.LimeTabularExplainer(np.array(x_train), feature_names = feature_names, class_names=['True', 'False'], discretize_continuous=True)

def data_inverse(l, data_row, num_samples):
    """Generates a neighborhood around a prediction.

    For numerical features, perturb them by sampling from a Normal(0,1) and
    doing the inverse operation of mean-centering and scaling, according to
    the means and stds in the training data. For categorical features,
    perturb by sampling according to the training distribution, and making
    a binary feature that is 1 when the value is the same as the instance
    being explained.

    Args:
        data_row: 1d numpy array, corresponding to a row
        num_samples: size of the neighborhood to learn the linear model

    Returns:
        A tuple (data, inverse), where:
            data: dense num_samples * K matrix, where categorical features
            are encoded with either 0 (not equal to the corresponding value
            in data_row) or 1. The first row is the original instance.
            inverse: same as data, except the categorical features are not
            binary, but categorical (as the original data)
    """
    data = np.zeros((num_samples, data_row.shape[0]))
    categorical_features = range(data_row.shape[0])
    if l.discretizer is None:
        data = l.random_state.normal(
            0, 1, num_samples * data_row.shape[0]).reshape(
            num_samples, data_row.shape[0])
        if l.sample_around_instance:
            data = data * l.scaler.scale_ + data_row
        else:
            data = data * l.scaler.scale_ + l.scaler.mean_
        categorical_features = l.categorical_features
        first_row = data_row
    else:
        first_row = l.discretizer.discretize(data_row)
    data[0] = data_row.copy()
    inverse = data.copy()
    for column in categorical_features:
        values = l.feature_values[column]
        freqs = l.feature_frequencies[column]
        inverse_column = l.random_state.choice(values, size=num_samples,
                                                  replace=True, p=freqs)
        binary_column = np.array([1 if x == first_row[column]
                                  else 0 for x in inverse_column])
        binary_column[0] = 1
        inverse_column[0] = data[0, column]
        data[:, column] = binary_column
        inverse[:, column] = inverse_column
    if l.discretizer is not None:
        inverse[1:] = l.discretizer.undiscretize(inverse[1:])
    inverse[0] = data_row
    return data, inverse

model = pickle.load(open('/content/drive/My Drive/classifier.sav', 'rb'))

a = data_inverse(l, x_train.iloc[0], 1000)

gm = GaussianMixture(n_components=2, covariance_type = 'diag')

p_data_train = a[1]

p_data_test = list(map(lambda x: x>=0.5, model.predict(p_data_train)))

gm.fit(p_data_train, p_data_test)

components = gm.precisions_

ind = []
target = gm.predict(x_train.iloc[0].values.reshape(1,-1))[0]
for i in range(len(p_data_train)):
    if gm.predict(p_data_train[i].reshape(1,-1))[0]==target:
        ind.append(i)

feature_count = np.zeros(135)

for i in ind:
    feature_count = list(map(int, p_data_train[i]!=0))+feature_count

def fidelity_test1(model, x_train, feature_count):
    '''
    If features Fx are accurately selected, then removing Fx from
    the input x will lead to classifying this image to a different
    label, i.e., “shoe” 
        '''
    o = []
    n = []
    new_prob = []
    for x_train_point in x_train.values:
        original = model.predict(x_train_point.reshape(1,-1))>=0.5
        len_ind = 0
        ma = max(feature_count)
        for i in range(len(feature_count)):
            if feature_count[i] == ma and x_train_point[i]!=0:
                x_train_point[i] = 0
                len_ind += 1
            if len_ind == 30:
                break
        np = model.predict(x_train_point.reshape(1,-1))
        new_prob.append(np)
        o.append(original)
        n.append(np>=0.5)
    return o, n, new_prob

def fidelity_test2(model, x_train, feature_count):
    '''
    If features Fx are accurately selected, then adding the feature values of Fx
    to an image of “shoe” is likely to lead to a
    misclassification, i.e., classifying it as a “sweater”
    '''
    new_prob = []
    orig_prob = []
    len_ind = 0
    o = []
    n = []
    ma = max(feature_count)
    ind = []
    for i in range(len(feature_count)):
        #feature_count[i] == ma and 
        if x_train.values[0][i]==0 and feature_count[i] == ma-200:
            ind.append(i)
            len_ind += 1
        if len_ind == 30:
            break
    for x_train_point in x_train.values:
        original = model.predict(x_train_point.reshape(1,-1))
        op = original
        original = original<=0.5
        if original==True:
            original = 1
        else:
            original = 0
        if sum(x_train_point[ind]==0)==len_ind and original!=target:
            for i in range(len_ind):
                x_train_point[ind[i]] = feature_maxs[ind[i]]
            new_predict = model.predict(x_train_point.reshape(1,-1))
            new_prob.append(new_predict)
            new_predict = new_predict>=0.5
            orig_prob.append(op)
            o.append(original)
            n.append(new_predict)

    return new_prob, new_prob, o, n

def fidelity_test3(model, feature_count):
    '''
    If features Fx are accurately selected, we can craft a synthetic
    images that only contains the features in Fx, and this synthetic 
    image is likely to be classified as “sweater”
    '''
    len_ind = 0
    o = []
    n = []
    ma = max(feature_count)
    ind = []
    for i in range(len(feature_count)):
        if feature_count[i] == ma:
            ind.append(i)
            len_ind += 1
        if len_ind == 30:
            break
    x_train_points = [0]*len(feature_count)
    for i in ind:
        x_train_points[i] = feature_means[i]
    return model.predict(np.array(x_train_points).reshape(1,-1))

o1, n1, n_prob = fidelity_test1(model, x_train, feature_count)
orig_prob, ft2_prob, o2, n2 = fidelity_test2(model, x_train, feature_count)
# model.predict(x_train.iloc[0].reshape(1,-1))

# print (ft2_prob, o2, n2)

print (model.predict(x_train.iloc[0].reshape(1,-1)))

# o, n = fidelity_test2(model, x_train, feature_count)
# print (sum(np.array(o)^np.array(n)))
print (fidelity_test3(model, feature_count))
# model.predict(x_train.iloc[0].reshape(1,-1))

# model = Sequential()
# model.add(Dense(1024, input_dim=len(x.columns), activation='relu'))
# model.add(Dense(1024, activation='relu'))
# model.add(Dense(1024, activation='sigmoid'))
# model.add(Dense(1, activation='sigmoid'))
# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# model.fit(x_train,y_train, epochs=20, batch_size=10)
# y_pred = model.predict(x_test)
# y_pred = list(map(lambda x: x>=0.5, y_pred))
# accuracy_score(y_test, y_pred)
# pickle.dump(model, open('classifier.sav', 'wb'))
# from google.colab import files
# files.download("classifier.sav")
# a = [1,1,1,1,1,1,1,1,1,1]
# b = [1,2,3,4]
# # a = np.array(a)
# a = x_train.values[0]
# b = np.array(b)
# sum(a[b]!=0)

y_pred=model.predict(p_data_train)

gm_pred1=[]
gm_pred2=[]
def LocalApproxAccuracy():      
    for i in range(len(y_pred)):
        gm_pred1.append(gm.predict_proba(p_data_train)[i][1])
        gm_pred2.append(gm.predict_proba(p_data_train)[i][0])
    rmse1=mean_squared_error(y_pred,gm_pred1)
    rmse2=mean_squared_error(y_pred,gm_pred2)
    return(min(rmse1,rmse2))

#LIME Prediction 
pred1=[]
for i in range(len(y_pred)):
    pred1.append(l.explain_instance(p_data_train[i],gm.predict_proba, num_features=5).predict_proba[0])
    print (i)
rmse=mean_squared_error(y_pred,pred1)

LocalApproxAccuracy()

def precision(ytrue,ypred):
    return precision_score(ytrue,ypred)
def recall(ytrue,ypred):
    return recall_score(ytrue,ypred)
def roc(ytrue,yscore):         
    fpr, tpr, thresholds = roc_curve(ytrue,yscore)
    plt.figure()
#     print (fpr[2], tpr[2], thresholds, sep='\n')
    plt.plot(fpr, tpr, color='darkorange')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic ')
    plt.show()
def confusionMatrix(ytrue,ypred):
    return confusion_matrix(ytrue,ypred)

for i in range(len(n2)):
    n2[i] = n2[i][0][0]

range(len(o1)):
    o1[i]=o1[i][0][0]
    n1[i]=n1[i][0][0]

print(precision(o1,n1))
print(recall(o1,n1))
print (confusionMatrix(o1, n1))

for i in range(len(o1)):
    if o1[i]==True:
        o1[i]=1
    else:
        o1[i]=0
for i in range(len(n_prob)):
    n_prob[i] = n_prob[i][0][0]

roc(o1, n_prob)

for i in range(len(o2)):
    n2[i] = n2[i][0][0]

print(precision(o2,n2))
print(recall(o2,n2))
print (confusionMatrix(o2, n2))

for i in range(len(ft2_prob)):
    ft2_prob[i] = ft2_prob[i][0][0]

print(roc(o2,ft2_prob))